# Study Project Home

Welcome to my Pyspark Project Home directory repository.

This repository contains code and resources for my PySpark code study project, where I explore various concepts of Apache Spark and related technologies.

## Overview

### Virtual Environment
I use **Conda** virtual environment from **Anaconda3** to create an isolated environment for my project, ensuring all dependencies are handled properly and do not interfere with other projects.

### PySpark Code Study
The **Home Folder** contains all the PySpark code for my studies, including exercises, examples, and tests related to **Apache Spark** programming.

To run the code in this folder, you need to install **Apache Spark**. Follow the installation guide below to set it up.

1. **Download and Install Apache Spark:**
   - Go to the official [Apache Spark website](https://spark.apache.org/downloads.html) and download the version that fits your operating system.

2. **Set up Spark:**
   - Follow the installation guide from the official [Spark documentation](https://spark.apache.org/docs/latest/) to set up Spark on your local machine.

3. **Install Dependencies:**
   - You can install PySpark via `pip` in your virtual environment:
     ```bash
     pip install pyspark
     ```

4. **Verify Installation:**
   - To verify if Spark is installed correctly, run the following command in your terminal:
     ```bash
     spark-shell
     ```

5. **Set Up Environment Variables (Optional):**
   - You might need to set some environment variables for Spark to run smoothly:
     - `SPARK_HOME` should point to your Spark installation directory.
     - `JAVA_HOME` should point to your Java installation directory.

   For more detailed instructions, check the official installation guide in the [Spark documentation](https://spark.apache.org/docs/latest/).

## Purpose

This repository aims to help you understand how Apache Spark works and how to set up an environment for Spark code development, especially when working with PySpark.

## How to Use This Repository

Feel free to use this repository for ideas and inspiration on creating your Apache Spark programming code.
